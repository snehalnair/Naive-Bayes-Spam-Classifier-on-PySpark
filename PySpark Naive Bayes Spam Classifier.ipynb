{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayesian_Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset can be downloaded from [UCI Machine Learning Respository](https://archive.ics.uci.edu/ml/machine-learning-databases/00228/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instantiate a spark session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName('SpamClassifier').getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+--------------------+\n",
      "|label_string|                 sms|\n",
      "+------------+--------------------+\n",
      "|         ham|Go until jurong p...|\n",
      "|         ham|Ok lar... Joking ...|\n",
      "|        spam|Free entry in 2 a...|\n",
      "|         ham|U dun say so earl...|\n",
      "|         ham|Nah I don't think...|\n",
      "|        spam|FreeMsg Hey there...|\n",
      "|         ham|Even my brother i...|\n",
      "|         ham|As per your reque...|\n",
      "|        spam|WINNER!! As a val...|\n",
      "|        spam|Had your mobile 1...|\n",
      "+------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load data and rename column\n",
    "df = spark.read.option(\"header\", \"false\") \\\n",
    "    .option(\"delimiter\", \"\\t\") \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .csv(\"SMSSpamCollection.txt\") \\\n",
    "    .withColumnRenamed(\"_c0\", \"label_string\") \\\n",
    "    .withColumnRenamed(\"_c1\", \"sms\")\n",
    "\n",
    "df.limit(10).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline Stages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import CountVectorizer\n",
    "from pyspark.ml.feature import Tokenizer, RegexTokenizer\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.classification import NaiveBayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " RegexTokenizer_00eb895e47f0\n",
      "\n",
      " CountVectorizer_017c1dd11567\n",
      "\n",
      " StringIndexer_92fa251d3fe5\n",
      "\n",
      " VectorAssembler_68253f194cda\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[None, None, None, None]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stages = []\n",
    "# 1. clean data and tokenize sentences using RegexTokenizer\n",
    "regexTokenizer = RegexTokenizer(inputCol=\"sms\", outputCol=\"tokens\", pattern=\"\\\\W+\")\n",
    "stages += [regexTokenizer]\n",
    "\n",
    "# 2. CountVectorize the data\n",
    "cv = CountVectorizer(inputCol=\"tokens\", outputCol=\"token_features\", minDF=2.0)#, vocabSize=3, minDF=2.0\n",
    "stages += [cv]\n",
    "\n",
    "# 3. Convert the labels to numerical values using binariser\n",
    "indexer = StringIndexer(inputCol=\"label_string\", outputCol=\"label\")\n",
    "stages += [indexer]\n",
    "\n",
    "# 4. Vectorise features using vectorassembler\n",
    "vecAssembler = VectorAssembler(inputCols=['token_features'], outputCol=\"features\")\n",
    "stages += [vecAssembler]\n",
    "\n",
    "[print('\\n', stage) for stage in stages]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "pipeline = Pipeline(stages=stages)\n",
    "data = pipeline.fit(df).transform(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split dataset into train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = data.randomSplit([0.7, 0.3], seed = 2018)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import NaiveBayes\n",
    "nb = NaiveBayes(smoothing=1.0, modelType=\"multinomial\")\n",
    "model = nb.fit(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+------------------------------------------+\n",
      "|label|prediction|probability                               |\n",
      "+-----+----------+------------------------------------------+\n",
      "|0.0  |0.0       |[0.9999996176179956,3.823820044882337E-7] |\n",
      "|0.0  |0.0       |[0.9972054995602091,0.002794500439790882] |\n",
      "|0.0  |0.0       |[0.9999999999978098,2.190326444063966E-12]|\n",
      "|0.0  |0.0       |[0.9999999999999538,4.607804951342392E-14]|\n",
      "|0.0  |0.0       |[0.999999999880886,1.1911406870203127E-10]|\n",
      "|0.0  |0.0       |[0.999688852925206,3.1114707479388615E-4] |\n",
      "|0.0  |0.0       |[0.9999999098737272,9.012627286140461E-8] |\n",
      "|0.0  |0.0       |[0.9999950690131734,4.930986826665776E-6] |\n",
      "|0.0  |0.0       |[0.9999795625725587,2.043742744135259E-5] |\n",
      "|0.0  |0.0       |[0.9999063364041348,9.366359586510845E-5] |\n",
      "+-----+----------+------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions = model.transform(test)\n",
    "# Select results to view\n",
    "predictions.limit(10).select(\"label\", \"prediction\", \"probability\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Accuracy:  0.972052252090383\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "evaluator = BinaryClassificationEvaluator(rawPredictionCol=\"prediction\")\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print (\"Model Accuracy: \", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experimenting with Various Smoothing Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9697048342500075"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "\n",
    "# Create ParamGrid and Evaluator for Cross Validation\n",
    "paramGrid = ParamGridBuilder().addGrid(nb.smoothing, [0.0, 0.2, 0.4, 0.6, 0.8, 1.0, 1.5, 2.0]).build()\n",
    "cvEvaluator = BinaryClassificationEvaluator(rawPredictionCol=\"prediction\")\n",
    "# Run Cross-validation\n",
    "cv = CrossValidator(estimator=nb, estimatorParamMaps=paramGrid, evaluator=cvEvaluator)\n",
    "cvModel = cv.fit(train)\n",
    "# Make predictions on testData. cvModel uses the bestModel.\n",
    "cvPredictions = cvModel.transform(test)\n",
    "# Evaluate bestModel found from Cross Validation\n",
    "evaluator.evaluate(cvPredictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Accuracy:  0.9697048342500075\n"
     ]
    }
   ],
   "source": [
    "# Make predictions on testData. cvModel uses the bestModel.\n",
    "cvPredictions = cvModel.transform(test)\n",
    "# Evaluate bestModel found from Cross Validation\n",
    "print (\"Model Accuracy: \", evaluator.evaluate(cvPredictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Turns out that smoothing has no effect on this dataset!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.3 64-bit ('imageenv': venv)",
   "language": "python",
   "name": "python37364bitimageenvvenv1bd32f4f13d047a990e9b995ab17fcf9"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
